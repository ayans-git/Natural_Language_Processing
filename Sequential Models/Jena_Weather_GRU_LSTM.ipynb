{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "data_dir = 'C:/Users/write/OneDrive/Deep Learning Data/jena_climate_2009_2016'\n",
    "fname = os.path.join(data_dir, 'jena_climate_2009_2016.csv')\n",
    "\n",
    "f = open(fname)\n",
    "data = f.read()\n",
    "f.close()\n",
    "\n",
    "lines = data.split('\\n')\n",
    "header = lines[0].split(',')\n",
    "samples = lines[1:]\n",
    "#print('Header:\\n', header)\n",
    "#print('Number of samples: ', len(samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "float_records = np.zeros((len(samples), len(header) - 1)) #exclude \"Date Time\" header\n",
    "for line_num, record in enumerate(samples):\n",
    "    float_records[line_num, : ] = [float(x) for x in record.split(',')[1:]] #exclude \"Date Time\" column entries\n",
    "\n",
    "mean = float_records[: 200000].mean(axis = 0)\n",
    "float_records -= mean\n",
    "std = float_records[: 200000].std(axis = 0)\n",
    "float_records /= std\n",
    "\n",
    "#print(float_records.shape[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Based on 10 days data can we predict next 24 hrs temperatures?\n",
    "#lookback = 1440 observations (6 * 24 * 10) \n",
    "#steps = 6 observations will be sampled at one data point every hr\n",
    "#delay = 144 Targets will be 24 hrs in the future\n",
    "#We will use first 200,000 as training data\n",
    "#At this point first 200,000 records in float_data collection are normalized and can be used for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Data Generator\n",
    ">It yields a tuple (samples, targets) where samples is one batch of input data\n",
    ">and targets us the corresponding array of target temperatures\n",
    "\n",
    "> data - The original array of floating-point data (float_records)\n",
    "> lookback - Defines how many timesteps back the input data should go (720 means 5 days, 1440 means 10 days, etc.)\n",
    "> delay - Defines how many timesteps in the future target should be (144 means 24 hrs given 1 record / 10 mins)\n",
    "> min_index and max_index - Indices in the data array [float_data] that delimit which timesteps to draw from. \n",
    "Helps in data segmentation for training, validation and text.\n",
    "Note, we normalized first 200,000 records for training\n",
    "> shuffle - Whether to shuffle the samples or draw them in chronological order. Usually validation and test data are not shuffled in Timeseries problem\n",
    "> batch_size - The number of samples per batch\n",
    "> step - The period, in timesteps, at which data is sampled. Here the data is sampled every 10 mins i.e. 6/hr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(data, lookback, delay, min_index, max_index, shuffle = False, batch_size = 128, step = 6):\n",
    "    if max_index is None:\n",
    "        max_index = ((len(data) - 1) - delay) #len() starts from 1 so for index need to -1 as index starts from 0\n",
    "    \n",
    "    lower_index = min_index + lookback\n",
    "    \n",
    "    while 1:\n",
    "        if shuffle:\n",
    "            rows = np.random.randint(lower_index, max_index, size = batch_size)\n",
    "        else:\n",
    "            if lower_index + batch_size >= max_index:\n",
    "                lower_index = min_index + lookback\n",
    "            rows = np.arange(lower_index, min(lower_index + batch_size, max_index))\n",
    "            lower_index += len(rows)\n",
    "        \n",
    "        samples = np.zeros((len(rows), lookback // step, data.shape[-1]))  #data.shape[-1] = 14\n",
    "        targets = np.zeros((len(rows),))\n",
    "        \n",
    "        for j, row in enumerate(rows):\n",
    "            indices = range(rows[j] - lookback, rows[j], step)\n",
    "            samples[j] = data[indices]\n",
    "            targets[j] = data[rows[j] + delay][1]  #At index 1 we have \"T (degC)\"\n",
    "        \n",
    "        yield samples, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookback = 1440 # 6 * 24 * 10 => 10 days\n",
    "step = 6 #number of observations per hour\n",
    "delay = 144 #target => 6 * 24 => predicting 24 hrs in future\n",
    "batch_size = 128\n",
    "\n",
    "train_gen = generator(float_records,\n",
    "                     lookback = lookback,\n",
    "                     delay = delay,\n",
    "                     min_index = 0,\n",
    "                     max_index = 200000, #first 200000 for training (normalized for training)\n",
    "                     shuffle = True, #training data is shuffled\n",
    "                     batch_size = batch_size, \n",
    "                     step = step\n",
    "                     )\n",
    "\n",
    "#Not shuffling validation and test data\n",
    "validation_gen = generator(float_records,\n",
    "                          lookback = lookback,\n",
    "                          delay = delay,\n",
    "                          min_index = 200001,\n",
    "                          max_index = 300000,\n",
    "                          batch_size = batch_size, \n",
    "                          step = step\n",
    "                          )\n",
    "\n",
    "test_gen = generator(float_records,\n",
    "                    lookback = lookback,\n",
    "                    delay = delay,\n",
    "                    min_index = 300001,\n",
    "                    max_index = None,\n",
    "                    batch_size = batch_size,\n",
    "                    step = step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.mean(batch_maes) = 0.28969941979609765\n",
    "#celsius_mae = 2.5645638478601653\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "from keras import losses\n",
    "\n",
    "val_steps = (300000 - 200001) - lookback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gru = Sequential()\n",
    "model_gru.add(layers.GRU(32, input_shape = (None, float_records.shape[-1])))\n",
    "model_gru.add(layers.Dense(1))\n",
    "\n",
    "model_gru.compile(optimizer = optimizers.RMSprop(),\n",
    "                 loss = losses.MAE)\n",
    "\n",
    "history_gru = model_gru.fit_generator(train_gen,\n",
    "                                     steps_per_epoch = 500,\n",
    "                                     epochs = 20,\n",
    "                                     validation_data = validation_gen,\n",
    "                                     validation_steps = val_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm = Sequential()\n",
    "model_lstm.add(layers.LSTM(32, input_shape(None, float_records.shape[-1])))\n",
    "model_lstm.add(layers.Dense(1))\n",
    "\n",
    "model_lstm.compile(optimizer = optimizers.RMSprop(),\n",
    "                  loss = losses.MAE)\n",
    "\n",
    "history_lstm = model_lstm.fit_generator(train_gen,\n",
    "                                       steps_per_epoch = 500,\n",
    "                                       validation_data = validation_gen,\n",
    "                                       validation_steps = val_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "gru_loss = history_gru.history['loss']\n",
    "gru_val_loss = history_gru.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(gru_loss) + 1)\n",
    "\n",
    "plt.plot(epochs, gru_loss, 'bo', title = 'Training Loss')\n",
    "plt.plot(epochs, gru_val_loss, 'b', title = 'Validation Loss')\n",
    "plt.title('GRU Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "lstm_loss = history_lstm.history['loss']\n",
    "lstm_val_loss = history_lstm.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(lstm_loss) + 1)\n",
    "\n",
    "plt.plot(epochs, lstm_loss, 'c^', title = 'Training Loss')\n",
    "plt.plot(epochs, lstm_val_loss, r, title = 'Validation Loss')\n",
    "plt.title('LSTM Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
